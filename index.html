<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="Lingshu">
    <meta name="keywords" content="Lingshu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Lingshu</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <!-- <link rel="icon" href="./static/images/favicon-32x32.png"> -->
    <link rel="icon" href="./static/images/seallm-audio-logo.png"> <!--- Change the Logo! --->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <!-- <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script> -->
    <!-- <script src="./static/js/index.js"></script> -->
    <script src="./static/js/custom.js"></script>
    <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js">
    </script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
 
</head>
<body>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-widescreen">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">
                        <img src="lingshu_logo.png" alt="" style="height: 2.5em;">
                        <br>
                        Lingshu: A Generalist Foundation Model for Unified <br> Multimodal Medical Understanding and Reasoning
                    </h1>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://liuchaoqun.github.io/">Chaoqun Liu</a>,
            </span>
            <span class="author-block">
                <a href="https://www.researchgate.net/scientific-contributions/Sharifah-Mahani-Aljunied-2178541347">Mahani Aljunied</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=HAdzwTsAAAAJ">Guizhen Chen</a>,
              </span>
            <span class="author-block">
                <a href="https://kenchan0226.github.io/">Hou Pong Chan</a>,
              </span>
              <span class="author-block">
                <a href="https://wwxu21.github.io/">Weiwen Xu</a>,
              </span>
                </div>
                <div class="is-size-5 publication-authors">
            
              <span class="author-block">
              <a href="https://royrong.me/">Yu Rong</a>,
            </span>
            <span class="author-block">
              <a href="https://isakzhang.github.io">Wenxuan Zhang*</a> (Corresponding Author)
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <br>
                        <span class="author-block">DAMO Academy, Alibaba Group</span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2503.00865"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span> -->
                </a>
              </span>
                <span class="link-block">
                <a href="https://alibaba-damo-academy.github.io/lingshu/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                        <i class="fas fa-globe"></i>
                  </span>
                  <span>Website</span>
                  </a>
                </span>
                <span class="link-block">
                <a href="https://arxiv.org/pdf/2506.07044"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Report</span>
                  </a>
                </span>
                            <!-- Code Link. -->
                <span class="link-block">
                <a href="https://github.com/alibaba-damo-academy/MedEvalKit"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>MedEvalKit</span>
                  </a>
              </span>
                            <!-- Models Link. -->
                <span class="link-block">
                <a href="https://huggingface.co/lingshu-medical-mllm/Lingshu-7B"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      ðŸ¤—
                  </span>
                  <span>Models</span>
                  </a>
                </span>
                
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="hero teaser">
    <!-- <div class="container">
        <div class="text-box">
            <p><em>Put a Quote here, if here.</em></p>
            <div class="quote-source">-- <em>Story from Genesis, Old Testament</em></div>
        </div>
    </div> -->
    <style>
        body {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            background-color: #f9f9f9;
            margin: 0;
        }

        .text-box {
            width: 100%; /* Thinner box */
            max-width: 1200px;
            margin: 1rem auto;
            padding: 20px;
            background-color: #f2f2f2;
            border: 1px solid black;
            border-radius: 15px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3);
            text-align: center;
            line-height: 1.8; /* Slightly increased for readability with larger text */
            font-size: 1.5rem; /* Increased base font size for larger text */
        }

        .text-box em {
            font-style: italic;
        }

        .text-box strong {
            font-weight: bold;
        }

        .quote-source {
            margin-top: 1em;
            font-style: italic;
            font-size: 1.0rem; /* Increased font size for the quote source */
        }

        .hero {
            padding: 1rem 1rem;
        }

        .hero-body {
            text-align: center;
        }
    </style>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <!-- <h2 class="title is-3">Abstract</h2> -->
                <div class="content has-text-justified">
                     <p>
                        <b>Welcome to the Lingshu project</b>. 
                    </p>
                    <ul>
                        <li>We show our SOTA multimodal large language models for the medical domain, <strong>Lingshu</strong>. </li>
                        <li>We release a unified evaluation framework, <strong>MedEvalKit</strong>, that consolidates major benchmarks for multimodal and textual medical tasks.</li>
                    </ul>
                    <p>
                        <b>Highlights</b>:
                    </p>
                    <ul>
                        <li>Lingshu supports more than 12 medical imaging modalities, including X-Ray, CT Scan, MRI, Microscopy, Ultrasound, Histopathology, Dermoscopy, Fundus, OCT, Digital Photography, Endoscopy, and PET.</li>
                        <li>Lingshu models achieve SOTA on most medical multimodal/textual QA and report generation tasks for 7B and 32 model sizes. Lingshu-32B outperforms GPT-4.1 and Claude Sonnet 4 in most multimodal QA and report generation tasks.</li>
                        <li>MedEvalKit allows for standardized, fair, and easy-to-use model assessment in medical domains, including multimdaol VQA, textual QA, and report generation. </li>
                    </ul>
                    <p>
                        <b>Quick Links</b>:
                    </p>
                    <ul>
                        <li><strong>Models</strong>: Model weights are available in multiple model sizes: <a href="https://huggingface.co/lingshu-medical-mllm/Lingshu-7B">Lingshu-7B</a>, <a href="https://huggingface.co/lingshu-medical-mllm/Lingshu-32B">Lingshu-32B</a></li>
                        <li><strong>Evaluation Toolkit</strong>: We release our evaluation framework: <a href="https://github.com/alibaba-damo-academy/MedEvalKit">MedEvalKit</a> </li>
                        <li><strong>Technique Report</strong>: Our technique report for the whole project is available at <a href="https://arxiv.org/pdf/2506.07044">Lingshu Report</a> . </li>
                    </ul>
                        
                    
                </div>
            </div>
        </div>
        <!--/ Abstract. -->

    </div>
</section>

<section class="section">
    <div class="container" style="max-width: 800px; margin: 0 auto;">
        <h2 class="title is-4">Performance: SeaBench-Audio</h2>
        <style>
            body {
                font-family: Arial, sans-serif;
            }

            .section {
                padding: 20px;
            }

            .container {
                max-width: 1000px;
                margin: 0 auto;
            }

            .title {
                font-size: 1.5rem;
                font-weight: bold;
                margin-bottom: 10px;
            }

            .content {
                margin-top: 10px;
            }

            .has-text-justified {
                text-align: justify;
            }

            .table-container {
                overflow-x: auto;
                margin-top: 20px;
            }

            table {
                width: 100%;
                border-collapse: collapse;
                margin-bottom: 20px;
            }

            th, td {
                border: 1px solid #ddd;
                padding: 8px;
                text-align: left;
            }

            th {
                background-color: #f4f4f4;
                font-weight: bold;
            }

            .highlight {
                background-color: #d9f2ff; /* Light blue for highlighted rows */
            }
        </style>
        <div class="content has-text-justified">
            <p>
                Due to the absence of standard audio benchmarks for evaluating audio LLMs in Southeast Asia, we have manually created a benchmark called <strong>SeaBench-Audio</strong>. It comprises nine tasks:
            </p>
            <ul>
                <li>
                    <strong>Tasks with both audio and text inputs:</strong> Audio Captioning (AC), Automatic Speech Recognition (ASR), Speech-to-Text Translation (S2TT), Speech Emotion Recognition (SER), Speech Question Answering (SQA), and Speech Summarization (SS).
                </li>
                <li>
                    <strong>Tasks with only audio inputs:</strong> Factuality, Math, and General.
                </li>
            </ul>
            <p>
                We manually annotated 15 questions per task per language. For evaluation, qualified native speakers rated each response on a scale of 1 to 5, with 5 representing the highest quality.  
            </p>
            Due to the lack of LALMs for all the three Southeast Asian languages, we compare the performance of SeaLLMs-Audio with relevant audio LLMs with similar sizes, including 
            <a href="https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct">Qwen2-Audio-7B-Instruct</a> (Qwen2-Audio), 
            <a href="https://huggingface.co/MERaLiON/MERaLiON-AudioLLM-Whisper-SEA-LION">MERaLiON-AudioLLM-Whisper-SEA-LION</a> (MERaLiON),
            <a href="https://huggingface.co/scb10x/llama3.1-typhoon2-audio-8b-instruct">llama3.1-typhoon2-audio-8b-instruct</a> (typhoon2-audio), and
            <a href="https://huggingface.co/WillHeld/DiVA-llama-3-v0-8b">DiVA-llama-3-v0-8b</a> (DiVA). 
            All the audio LLMs can accept audio with text as input. The results are shown in the figure below.
            
        </div>
        <div style="text-align: center;">
            <figure>
                <figcaption>Average scores of SeaLLMs-Audio vs. Other LALMs</figcaption>
                <img src="./static/images/scores_lang.png" alt="Performance of SeaLLMs-Audio vs. Other Audio LLMs">
            </figure>
        </div>
        <p>The results shows that SeaLLMs-Audio achieve state-of-the-art performance in all the five langauges, demonstrating its effectiveness in supporting audio-related tasks in Southeast Asia.</p><br>
        <!-- Medical VQA Section -->
        <h2 class="title is-4">Medical Visual Question Answering</h2>
        <div style="text-align: center;">
            <figure>
                <figcaption>Average scores of SeaLLMs-Audio vs. Other LALMs</figcaption>
                <img src="table6.png" alt="Performance of SeaLLMs-Audio vs. Other Audio LLMs">
            </figure>
        </div>

        <!-- Medical Textual QA Section -->
        <h2 class="title is-4">Medical Textual Question Answering</h2>
        <div style="text-align: center;">
            <figure>
                <figcaption>Average scores of SeaLLMs-Audio vs. Other LALMs</figcaption>
                <img src="table7.png" >
            </figure>
        </div>

        <!-- Medical Report Generation Section -->
        <h2 class="title is-4">Medical Report Generation</h2>
        <div style="text-align: center;">
            <figure>
                <figcaption>Average scores of SeaLLMs-Audio vs. Other LALMs</figcaption>
                <img src="table8.png">
            </figure>
        </div>
        
        <!-- </div>

        </div> -->

        <h2 class="title is-4">Quick Start</h2>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 20px;
                line-height: 1.6;
            }
            code {
                background-color: #f4f4f4;
                padding: 5px;
                border-radius: 5px;
            }
            pre {
                background-color: #f4f4f4;
                padding: 10px;
                border-radius: 5px;
                overflow-x: auto;
            }
            pre code {
                display: block;
                white-space: pre;
            }
        </style>
        <div class="content has-text-justified">
        <p>Lingshu model is available on <a href="https://huggingface.co/lingshu-medical-mllm/Lingshu-7B">Hugging Face</a>. You can easily use it with the <b>transformers</b> library or <b>vllm</b> library. Below are some examples to get you started.</p>
        
        <h5>Get started with <b>transformers</b></h5>
        <pre><code class="language-python">from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info


# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    "lingshu-medical-mllm/Lingshu-32B",
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
    device_map="auto",
)

processor = AutoProcessor.from_pretrained("lingshu-medical-mllm/Lingshu-32B")

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "example.png",
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# Preparation for inference
text = processor.apply_chat_template(
    messages, tokenize=False, add_generation_prompt=True
)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
</code></pre>

<br>    
        <h5>Get started with <b>vllm</b></h5>
        <pre><code class="language-python">from vllm import LLM, SamplingParams
from qwen_vl_utils import process_vision_info
import PIL
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("lingshu-medical-mllm/Lingshu-32B")
llm = LLM(model="lingshu-medical-mllm/Lingshu-32B", limit_mm_per_prompt = {"image": 4}, tensor_parallel_size=2, enforce_eager=True, trust_remote_code=True,)
sampling_params = SamplingParams(
            temperature=0.7,
            top_p=1,
            repetition_penalty=1,
            max_tokens=1024,
            stop_token_ids=[],
        )

text = "What does the image show?"
image_path = "example.png"
image = PIL.Image.open(image_path)

message = [
    {
        "role":"user",
        "content":[
            {"type":"image","image":image},
            {"type":"text","text":text}
            ]
            }
]
prompt = processor.apply_chat_template(
    message,
    tokenize=False,
    add_generation_prompt=True,
)
image_inputs, video_inputs = process_vision_info(message)
mm_data = {}
mm_data["image"] = image_inputs
processed_input = {
  "prompt": prompt,
  "multi_modal_data": mm_data,
}

outputs = llm.generate([processed_input], sampling_params=sampling_params)
print(outputs[0].outputs[0].text)
            </code></pre>
        </div>

        
    </div>
    </div>
</section>


<!-- model information -->


<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <p>
            If you find our project useful, we hope you would kindly star our repo and cite our work as follows.
            <br>
            * `*` are equal contributions.  `^` are corresponding authors.
        </p>
        <pre><code>@misc{lasateam2025lingshu,
      title={Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning}, 
      author={LASA Team and Weiwen Xu* and Hou Pong Chan* and Long Li* and Mahani Aljunied and Ruifeng Yuan and Jianyu Wang and Chenghao Xiao and Guizhen Chen and Chaoqun Liu and Zhaodonghui Li and Yu Sun and Junao Shen and Chaojun Wang and Jie Tan and Deli Zhao and Tingyang Xu and Hao Zhang^ and Yu Rong^},
      year={2025},
      eprint={2506.07044},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.07044}, 
}</code></pre>
    </div>
</section>


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>,
                        licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
